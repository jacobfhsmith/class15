---
title: "CLT-based inference - confidence intervals"
author: ""
date: ""
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE,
                      comment = "#>", highlight = TRUE,
                      fig.align = "center")
```

## Main ideas

- Understand the CLT and how to use the result

- Create confidence intervals for the population mean using a CLT-based
  approach
  
- Create confidence intervals for the population proportion using a CLT-based
  approach
  
# Packages

```{r packages}
library(tidyverse)
library(infer)
```

# Variability of sample statistics

- Each sample from the population yields a slightly different sample 
  statistic (sample mean, sample proportion, etc.)

- The variability of these sample statistics is measured by the **standard error**

- Previously we quantified this value via simulation

- Today we'll discuss some of the theory underlying **sampling distributions**, particularly as they relate to *sample means*.

## Recall

Statistical inference is the act of generalizing from a sample in order to make conclusions regarding a population. As part of this process, we quantify the degree of certainty we have. 

We are interested in population parameters, which we do not observe. Instead, we must calculate statistics from our sample in order to learn about them.

## Sampling distribution of the mean

Suppose weâ€™re interested in the resting heart rate of students at Duke, and are able to do the following:

1. Take a random sample of size $n$ from this population, and calculate the 
   mean resting heart rate in this sample, $\bar{X}_1$

2. Put the sample back, take a second random sample of size $n$, and calculate the mean resting heart rate from this new sample, $\bar{X}_2$

3. Put the sample back, take a third random sample of size $n$, and calculate
   the mean resting heart rate from this sample, too...and so on.

After repeating this many times, we have a dataset that has the
sample averages from the population: $\bar{X}_1$, $\bar{X}_2$, $\cdots$,
$\bar{X}_K$ (assuming we took $K$ total samples).

## Sampling distribution of the mean

**Question**:Can we say anything about the distribution of these sample means?
*(Keep in mind, we don't know what the underlying distribution of mean resting heart rate looks like in Duke students!)*

As it turns out, we can...

## The Central Limit Theorem

For a population with a well-defined mean $\mu$ and standard deviation $\sigma$, these three properties hold for the distribution of sample average $\bar{X}$,assuming certain conditions hold:

1. The mean of the sampling distribution is identical to the population mean
$\mu$,

2. The standard deviation of the distribution of the sample averages is
$\sigma/\sqrt{n}$, or the **standard error** (SE) of the mean, and

3. For $n$ large enough (in the limit, as $n \to \infty$), the shape of the
sampling distribution of means is approximately *normal* (Gaussian).

## What is the normal (Gaussian) distribution?

The normal distribution is unimodal and symmetric and is described by its
*density function*:

If a random variable $X$ follows the normal distribution, then
$$f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left\{ -\frac{1}{2}\frac{(x - \mu)^2}{\sigma^2} \right\}$$
where $\mu$ is the mean and $\sigma^2$ is the variance. 

We often write $N(\mu, \sigma^2)$ to describe this distribution.

## The normal distribution (graphically)

We will talk about probability densities and using them to define probabilities later, but for now, just know that the normal 
distribution is the familiar "bell curve":

```{r normdist, echo = FALSE, fig.height=5}
x <- seq(-3, 3, 0.01)
y <- dnorm(x)
plot(x, y, 
     type = "l", 
     lwd = 2,
     xlab = "", 
     ylab = "Density, N(0, 1)")
```

## But we didn't know anything about the underlying distribution!

The central limit theorem tells us that sample averages are
normally distributed, if we have enough data. This is true even if
our original variables are not normally distributed.

[**Check out this interactive demonstration!**](http://onlinestatbook.com/stat_sim/sampling_dist/index.html)

## Conditions

What are the conditions we need for the CLT to hold?

- **Independence:** The sampled observations must be independent. This is 
difficult to check, but the following are useful guidelines:
    - the sample must be random
    - if sampling without replacement, sample size must be less than 10% of the population size
    
- **Sample size / distribution:** 
    - if data are numerical, usually n $\geq$ 30 is considered a large enough sample, but if the underlying population distribution is extremely skewed, more might be needed
    - if we know for sure that the underlying data are normal, then the 
    distribution of sample averages will also be exactly normal, regardless of
    the sample size
    - if data are categorical, at least 10 successes and 10 failures.

## Let's run our own simulation

**The underlying population** (we never observe this!)

```{r pop}
rs_pop <- tibble(x = rbeta(100000, 1, 5) * 100)
```

```{r figure, fig.height=6, echo=FALSE}
ggplot(data = rs_pop, aes(x = x)) +
  geom_histogram(binwidth = 5, fill = "blue", alpha = 0.8, color = "grey90") +
  labs(title = "Population distribution") +
  theme_minimal(base_size = 16) +
  theme(axis.title.y = element_blank(),
        axis.text.y  = element_blank(),
        axis.ticks.y = element_blank())
```

**The true population parameters**

```{r trueparameters}
rs_pop %>%
  summarise(mu = mean(x), sigma = sd(x))
```

## Sampling from the population - 1

```{r sample1}
samp_1 <- rs_pop %>%
  sample_n(size = 50, replace = TRUE)
```


```{r samplemean1}
samp_1 %>%
  summarise(x_bar = mean(x))
```

## Sampling from the population - 2

```{r sample2}
samp_2 <- rs_pop %>%
  sample_n(size = 50, replace = TRUE)
```

```{r samplemean2}
samp_2 %>%
  summarise(x_bar = mean(x))
```

## Sampling from the population - 3

```{r sample3}
samp_3 <- rs_pop %>%
  sample_n(size = 50, replace = TRUE)
```

```{r samplemean3}
samp_3 %>%
  summarise(x_bar = mean(x))
```

keep repeating...

## Sampling distribution

```{r samplingdist}
sampling <- rs_pop %>%
  rep_sample_n(size = 50, replace = TRUE, reps = 1000) %>%
  group_by(replicate) %>%
  summarise(xbar = mean(x))
```

```{r distributionplot, fig.height=6, echo=FALSE}
ggplot(data = sampling, aes(x = xbar)) +
  geom_histogram(binwidth = 1, fill = "red", alpha = 0.8, color = "grey90") +
  labs(title = "Sampling distribution of sample means",
       x = expression(bar(X))) +
  theme_minimal(base_size = 16) +
  theme(axis.title.y = element_blank(),
        axis.text.y  = element_blank(),
        axis.ticks.y = element_blank())
```

## Sampling distribution quantities

```{r quantities, echo=FALSE}
sampling %>%
  summarise(mean = mean(xbar), se = sd(xbar))
```

## Comparison

**Question**: How do the shapes, centers, and spreads of these distributions compare?
```{r comparedist, echo=FALSE, fig.height=6, fig.width=12}
p1 <- ggplot(data = rs_pop, aes(x = x)) +
  geom_histogram(binwidth = 5, fill = "blue", alpha = 0.8, color = "grey90") +
  labs(title = "Population distribution") +
  xlim(-5, 100) +
  theme_minimal(base_size = 16) +
  theme(axis.title.y = element_blank(),
        axis.text.y  = element_blank(),
        axis.ticks.y = element_blank())
p2 <- ggplot(data = sampling, aes(x = xbar)) +
  geom_histogram(binwidth = 1, fill = "red", alpha = 0.8, color = "grey90") +
  labs(title = "Sampling distribution of sample means",
       x = expression(bar(X))) +
  xlim(-5, 100) +
  theme_minimal(base_size = 16) +
  theme(axis.title.y = element_blank(),
        axis.text.y  = element_blank(),
        axis.ticks.y = element_blank())
p1 + p2
```

---

## Recap

- If certain assumptions are satisfied, **regardless of the shape of the 
  population distribution**, the sampling distribution of the mean follows an 
  approximately normal distribution.

- The center of the sampling distribution is at the center of the population 
  distribution.

- The sampling distribution is less variable than the population distribution 
  (and we can quantify by how much).

What is the standard error, and how are the standard error and sample size 
related? What does that say about how the spread of the sampling distribution
changes as $n$ increases?

We can use these new results to construct confidence intervals.

# Data

In the examples and practice sections, we'll work with a subset of data from 
the General Social Survey.

```{r read_data}
gss_2010 <- read_csv("~/gss_2010.csv")
```

# Notes

Remember that for a population with a well-defined mean $\mu$ and standard 
deviation $\sigma$, these three properties hold for the distribution of sample average $\bar{X}$, assuming certain conditions hold:

- The distribution of the sample statistic is nearly normal
- The distribution is centered at the unknown population mean
- The variability of the distribution is inversely proportional to the square
  root of the sample size.

Knowing the distribution of the sample statistic $\bar{X}$ can help us

- estimate a population parameter as point estimate $\pm$ margin of error, where the margin of error is comprised of a measure of how confident we want to be and the sample statistic's variability.

- test for a population parameter by evaluating how likely it is to obtain the
  observed sample statistic when assuming that the null hypothesis is true as 
  this probability will depend on the sampling distribution's variability.

## Normal distribution

If necessary conditions are met, we can also use inference methods based on the CLT. Then the CLT tells us that $\bar{X}$ approximately has the distribution $N\left(\mu, \sigma/\sqrt{n}\right)$. That is,

$$Z = \frac{\bar{X} - \mu}{\sigma/\sqrt{n}} \sim N(0, 1)$$
   
Visualize some normal densities

```{r base_viz, echo=FALSE}
ggbase <- ggplot() +
  xlim(-10, 10) +
  labs(y = "") +
  theme_bw()
```


```{r normal_viz, echo=FALSE}
ggbase +
  stat_function(fun = dnorm, args = list(mean = 0, sd = 1), 
                color = "red", size = 1.5) +
  stat_function(fun = dnorm, args = list(mean = 2, sd = 2), 
                color = "blue", size = 1.5) +
  stat_function(fun = dnorm, args = list(mean = 0, sd = 4), 
                color = "purple", size = 1.5) +
  stat_function(fun = dnorm, args = list(mean = -5, sd = 0.5), 
                color = "grey60", size = 1.5)
```

## t-distribution

While we can (and will) use the CLT result to do inference, in practice, we 
never know the true value of $\sigma$, and so we estimate it
from our data with $s$ (sample standard deviation). The quantity $T$
has a **t-distribution** with $n-1$ *degrees of freedom*:

$$ T = \frac{\bar{X} - \mu}{s/\sqrt{n}} \sim t_{n-1}$$

- The t-distribution is also unimodal and symmetric, and is centered at 0

- It has thicker tails than the normal distribution (to make up for additional 
  variability introduced by using $s$ instead of $\sigma$)
  
```{r t_viz, echo=FALSE}
ggbase +
  stat_function(fun = dnorm, args = list(mean = 0, sd = 1), 
                color = "grey60", size = 1.5) +
  stat_function(fun = dt, args = list(df = 1), 
                color = "blue", size = 1) +
  stat_function(fun = dt, args = list(df = 5), 
                color = "red", size = 1) +
  stat_function(fun = dt, args = list(df = 10), 
                color = "orange", size = 1) +
  stat_function(fun = dt, args = list(df = 30), 
                color = "violet", size = 1) +
  xlim(-4, 4)
```

What do you notice in the plot?

## Computing a confidence interval for $\mu$

Recall that in our bootstrap simulation-based approach to creating confidence
intervals, the last step was to calculate the bounds of the `XX%` confidence 
interval as the middle `XX%` of the bootstrap distribution. Rather than work
with the bootstrap distribution, we can work directly with the theoretical
sampling distribution of the sample statistic. We know this from the CLT.

To find cutoffs (quantiles) from the normal and t distributions, we can use 
functions `qnorm()` and `qt()`, respectively.

```{r cutoff_examples}
qnorm(p = 0.975, mean = 0, sd = 1)
qnorm(0.975)
qt(p = 0.975, df = 5)
qt(p = 0.975, df = 10)
qt(p = 0.975, df = 1000)
```

### Example: confidence interval for $\mu$

The GSS asks "After an average work day, about how many 
hours do you have to relax or pursue activities that you enjoy?". Compute a 95%
confidence interval for the mean hours of relaxation time per day after work
using a CLT-based approach.

First, we'll check out our sample data and compute some summary statistics.

```{r summary_stats_example}
hrs_relax_stats <- gss_2010 %>% 
  filter(!is.na(hrsrelax)) %>%
  summarise(x_bar = mean(hrsrelax), 
            s     = sd(hrsrelax), 
            n     = n())
hrs_relax_stats
```

#### Direct calculation via formula

Let's grab these three statistics as vectors to make it easier to compute our
confidence interval.

```{r stats_vectors_example}
n <- hrs_relax_stats$n
x_bar <- hrs_relax_stats$x_bar
s <- hrs_relax_stats$s
```

Our confidence interval formula is given by

$$\mbox{point estimate} \pm t^* \times \mbox{SE},$$

where our point estimate will be the sample mean, $t^*$ is the
cut value from the t-distribution corresponding to the desired confidence level,
and the standard error is a function of the sample standard deviation and sample 
size.

$$\bar{x} \pm t^* \times \frac{s}{\sqrt{n}}$$ 

```{r t_crit_example}
(t_crit <- qt(p = 0.975, df = n - 1))
```

Why do we have `p = 0.975`?

```{r ci_example}
x_bar + c(-1, 1) * t_crit * (s / sqrt(n))
```

How do we interpret this?

#### Infer

The `infer` package has a function to do these calculations in one
step. Function `t_test()` is a tidier version of the built-in R function
`t.test()`.

```{r infer}
t_test(gss_2010, response = hrsrelax, conf_level = 0.95)
```

For now, focus on the last two variables - `lower_ci` and `upper_ci`. Next
time we'll discuss the first four in our lecture on hypothesis testing.

### Assumptions and requirements

What assumptions must we make for this inference procedure to be valid?

1.

2.

## Practice

The built-in dataset `quakes` gives information on seismic events near Fiji
since 1964. 

(1) Take a random sample of 40 events from `quakes`. You can use 
    `dplyr`'s `slice_sample()`. Save this result as an object named
    `quakes_40`.

```{r practice_1}

```

(2) Compute some summary statistics from `quakes_40`.

```{r practice_2}

```

(3) Compute a 90% confidence interval for the mean depth of seismic activity
    near Fiji.
    
```{r practice_3}
```

(4) Give an interpretation of your interval.

(5) Assume `quakes` consists off all the seismic activity that every occurred
    near Fiji. Does you 90% confidence interval cover the population parameter?

```{r practice_5}

```

## Computing a confidence interval for $p$

Our sample proportion $\hat{p}$ is the most plausible value of the population
proportion, $p$, so it makes sense to build a confidence interval around this 
point estimate. The standard error provides a guide for how large we should make the confidence interval.

The standard error represents the standard deviation of the point estimate, and when the Central Limit Theorem conditions are satisfied, the point estimate closely follows a normal distribution. The CLT tells 
us that $\hat{p}$ approximately has the distribution 
$N\left(p, \sqrt{\frac{p(1-p)}{n}}\right)$.

To ensure our sample is "large" for proportions, we must verify the
success-failure condition:

1. $n\hat{p} \ge 10$
2. $n(1-\hat{p}) \ge 10$

A confidence interval for $p$ is given by

$$\hat{p} \pm z^* \times \sqrt{\frac{\hat{p}(1-\hat{p})}{n}},$$
where $z^*$ corresponds to the confidence level selected. Since we don't know
$p$ we make a substitution using $\hat{p}$ in our SE.

### Example: confidence interval for $p$

The GSS asks "Are you better off today than you were four years ago?". 
Compute a 95% confidence interval for the proportion of Americans that are
better off today than four years ago. Use a CLT-based approach.

First, we'll check the success-failure condition.

```{r success_failure_check}
gss_2010 %>% 
  count(better)
```

We're also assuming these observations are independent.

Let's compute our 95% confidence interval.

```{r better_ci}
gss_2010 %>% 
  mutate(better = ifelse(better == 1, "better", "worse")) %>% 
  prop_test(response = better, conf_level = 0.95, success = "better") %>% 
  select(ends_with("ci"))
```

## Practice

Redo the above analysis using the confidence interval formula directly, but 
this time create a 90% confidence interval.

```{r formula_ci90_p}

```

## For Next Class

 Before next lecture, read the following sections from [OpenIntro Statistics - Fourth Edition](http://www2.stat.duke.edu/courses/Spring21/sta199.003/books/openintro-statistics.pdf)
  - 6.1 Inference for a single proportion 
  
  - 6.2 Difference of two proportions
  
  - 7.1 One-sample means with the t-distribution
  
  - 7.3 Difference of two means
  
## References

1. "Infer - Tidy Statistical Inference". Infer.Netlify.App, 2021, 
   https://infer.netlify.app/index.html.